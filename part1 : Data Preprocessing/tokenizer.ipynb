{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c656576",
   "metadata": {
    "panel-layout": {
     "height": 77.875,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "## **Toknizer** \n",
    "### **Step 1 : Spliting The Text Into Indidual Tokens  (Words and Subwords)**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee566e6-3a6d-4f10-b117-bb732fb4e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5541d83",
   "metadata": {
    "panel-layout": {
     "height": 0,
     "visible": true,
     "width": 100
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as txt_file:\n",
    "       raw_data = txt_file.read()\n",
    "\n",
    "print(\"total characters:\", len(raw_data))\n",
    "print(raw_data[:250])  # print the first 250 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2240bd-8453-45c7-a3f6-99ee07d9d5e6",
   "metadata": {
    "panel-layout": {
     "height": 10,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "**now we get our dataset which is a simple novel we want to convert it to tokens** \n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "How can we best split this text to obtain a list of tokens? For this, we go on a small\n",
    "excursion and use Python's regular expression library re for illustration purposes. (Note\n",
    "that you don't have to learn or memorize any regular expression syntax since we will\n",
    "transition to a pre-built tokenizer later in this chapter.) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "708b38f5-8b14-42bb-84ae-34f9aba95100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '--', 'though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough', '--', 'so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ', 'that', ',', '', ' ', 'in', ' ', 'the', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory', ',', '', ' ', 'he', ' ', 'had', ' ', 'dropped', ' ', 'his', ' ', 'painting', ',', '', ' ', 'married', ' ', 'a', ' ', 'rich', ' ', 'widow', ',', '', ' ', 'and', ' ', 'established', ' ', 'himself', ' ', 'in', ' ', 'a', ' ']\n"
     ]
    }
   ],
   "source": [
    "toknized_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)' , raw_data )\n",
    "print(toknized_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d83aa4-17e1-4334-bd49-d140bec31b88",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db89eb4b-aec2-493e-8b25-00ee1181866b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "REMOVING WHITESPACES OR NOT\n",
    "\n",
    "\n",
    "When developing a simple tokenizer, whether we should encode whitespaces as\n",
    "separate characters or just remove them depends on our application and its\n",
    "requirements. Removing whitespaces reduces the memory and computing\n",
    "requirements. However, keeping whitespaces can be useful if we train models that\n",
    "are sensitive to the exact structure of the text (for example, Python code, which is\n",
    "sensitive to indentation and spacing). Here, we remove whitespaces for simplicity\n",
    "and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme\n",
    "that includes whitespaces.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "079dadd9-f106-40fc-b804-0d691b623e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter', '--']\n"
     ]
    }
   ],
   "source": [
    "#remove white spaces\n",
    "toknized_text = [word.strip() for word in toknized_text if word.strip()]\n",
    "print(toknized_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e6c3da-5c56-40f8-9506-5199563ebf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(toknized_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f600e22-6b16-4174-96ad-8d50a9185e9a",
   "metadata": {},
   "source": [
    "### **Step 2 : Creating Token IDs** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e1861e-5fb5-4dce-812a-8510279c7dc2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In the previous section, we tokenized the story and assigned it to a\n",
    "Python variable called preprocessed. Let's now create a list of all unique tokens and sort\n",
    "them alphabetically to determine the vocabulary size:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "281fb1f0-a0e5-4a50-baa9-2803caf6f594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = sorted(set(toknized_text)) \n",
    "number_of_unique=len(unique_words)\n",
    "number_of_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9b5734-e0e1-40be-b911-10b5bfb9b0d3",
   "metadata": {},
   "source": [
    "#### **Crating The Vocabulary Itself**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5a1f0cc-3a18-46e2-8e3a-4a80d5b0737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary ={token : ID for ID, token in enumerate(unique_words)} # this has each token and its id \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95ab8572-159d-4e0e-b372-83a646a6936b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, '\"': 1, \"'\": 2, '(': 3, ')': 4, ',': 5, '--': 6, '.': 7, ':': 8, ';': 9, '?': 10, 'A': 11, 'Ah': 12, 'Among': 13, 'And': 14, 'Are': 15, 'Arrt': 16, 'As': 17, 'At': 18, 'Be': 19, 'Begin': 20, 'Burlington': 21, 'But': 22, 'By': 23, 'Carlo': 24, 'Chicago': 25, 'Claude': 26, 'Come': 27, 'Croft': 28, 'Destroyed': 29, 'Devonshire': 30}\n"
     ]
    }
   ],
   "source": [
    "simple_show={}\n",
    "for key,val in vocabulary.items():\n",
    "    simple_show[key]=val\n",
    "    if val ==30:\n",
    "        break\n",
    "print(simple_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe190cbb-56ac-4f15-a259-7cd3d7b4e694",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Later in this book, when we want to convert the outputs of an LLM from numbers back into\n",
    "text, we also need a way to turn token IDs into text. \n",
    "\n",
    "For this, we can create an inverse\n",
    "version of the vocabulary that maps token IDs back to corresponding text tokens.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070e44d-a838-4335-9c28-156a0ff2529c",
   "metadata": {},
   "source": [
    "**Let's implement a complete tokenizer class in Python.**\n",
    "\n",
    "**The class will have an encode method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary.**\n",
    "\n",
    "**In addition, we implement a decode method that carries out the reverse integer-to-string mapping to convert the token IDs back into text.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f786f6ca-69be-48b1-b9b0-36a1c092b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizar:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encoder(self,raw_text):\n",
    "        toknized_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)' , raw_text )\n",
    "        toknized_text = [word.strip() for word in toknized_text if word.strip()]\n",
    "        #the last part is to convert tokens to IDs\n",
    "        token_IDs = [self.str_to_int[token] for token in toknized_text ]\n",
    "\n",
    "        return token_IDs\n",
    "          \n",
    "    def decoder(self,IDs):\n",
    "        text = \" \".join([self.int_to_str[ID] for ID in IDs])\n",
    "        #remove spaces before spitial chars \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        \n",
    "        return text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e3853b-f57f-4829-8980-9c36b1abdd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "#lets try the class from our vocab \n",
    "tokenizer = SimpleTokenizar(vocabulary)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encoder(text)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbc28a88-04d1-40a4-aa3a-0b7f4391bd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the orginal text : \"It's the last he painted, you know,\" \n",
      "           Mrs. Gisburn said with pardonable pride.\n",
      "the decoded text : \" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "# turn it back to text \n",
    "decoded_text = tokenizer.decoder(ids)\n",
    "print(\"the orginal text :\",text)\n",
    "print(\"the decoded text :\",decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715defce-82c0-49d0-8bc3-64307cd48e82",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We implemented a tokenizer capable of tokenizing and de-tokenizing\n",
    "text based on a snippet from the training set. \n",
    "\n",
    "Let's now apply it to a new text sample that\n",
    "is not contained in the training set:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "567c9b8e-9754-4fcc-b522-7027360ae980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorry the tokenizer didnt see this before!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    text = \"Hello, do you like tea?\"\n",
    "    print(tokenizer.encode(text))\n",
    "except:\n",
    "    print(\"sorry the tokenizer didnt see this before!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d22670-9158-4e64-a67c-905068374a37",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The problem is that the word \"Hello\" was not used in the The Verdict short story. \n",
    "\n",
    "Hence, it\n",
    "is not contained in the vocabulary. \n",
    "\n",
    "This highlights the need to consider large and diverse\n",
    "training sets to extend the vocabulary when working on LLMs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24a291b-9bad-46ba-95ee-ac404d46efdf",
   "metadata": {},
   "source": [
    "### ADDING SPECIAL CONTEXT TOKENS\n",
    "\n",
    "In the previous section, we implemented a simple tokenizer and applied it to a passage\n",
    "from the training set. \n",
    "\n",
    "In this section, we will modify this tokenizer to handle unknown\n",
    "words.\n",
    "\n",
    "\n",
    "In particular, we will modify the vocabulary and tokenizer we implemented in the\n",
    "previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and\n",
    "<|endoftext|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c37be-06c1-4248-abfe-93138ba1fc30",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We can modify the tokenizer to use an <|unk|> token if it\n",
    "encounters a word that is not part of the vocabulary. \n",
    "\n",
    "Furthermore, we add a token between\n",
    "unrelated texts. \n",
    "\n",
    "For example, when training GPT-like LLMs on multiple independent\n",
    "documents or books, it is common to insert a token before each document or book that\n",
    "follows a previous text source\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d9c96c6-0556-4069-bb68-1eaa482583d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = sorted(set(toknized_text))\n",
    "# add the 2 new tokens <|unk|> , <|endoftext|>\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "full_vocab = {token:ID for ID,token in enumerate(all_tokens)}\n",
    "\n",
    "len(full_vocab.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2b396-e652-488c-922d-1635b60b8850",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Based on the output of the print statement above, the new vocabulary size is 1132 (the\n",
    "vocabulary size in the previous section was 1130).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d52bb99e-86ef-4bd9-90fc-0cc71f04863c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(full_vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff44a39-80c9-4dbb-8fe3-b935d370f770",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "A simple text tokenizer that handles unknown words\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff7a3e2c-b575-4a24-866c-5f93841a7f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizarV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encoder(self,raw_text):\n",
    "        toknized_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)' , raw_text )\n",
    "        toknized_text = [word.strip() for word in toknized_text if word.strip()]\n",
    "        \n",
    "        #adding the part of adding unkown token \n",
    "        toknized_text = [\n",
    "            token if token in self.str_to_int \n",
    "            else \"<|unk|>\" for token in toknized_text \n",
    "        ]\n",
    "        \n",
    "        #the last part is to convert tokens to IDs\n",
    "        token_IDs = [self.str_to_int[token] for token in toknized_text ]\n",
    "\n",
    "        return token_IDs\n",
    "          \n",
    "    def decoder(self,IDs):\n",
    "        text = \" \".join([self.int_to_str[ID] for ID in IDs])\n",
    "        #remove spaces before spitial chars \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f101268-54bd-488d-a75b-b40893be4574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizarV2(full_vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93876967-7320-47b3-96d7-09dc93c030a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids=tokenizer.encoder(text)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66bdfd71-5d85-468c-a3ab-5bb1f1d2431f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decoder(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab01a6c-aa38-4d4c-93a9-df1998b33fa7",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Based on comparing the de-tokenized text above with the original input text, we know that\n",
    "the training dataset, Edith Wharton's short story The Verdict, did not contain the words\n",
    "\"Hello\" and \"palace.\"\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e48e2e1-bc29-4cfe-bc9d-9ad76fd11284",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding (BPE) \n",
    "\n",
    "## What is BPE?\n",
    "Byte-Pair Encoding (BPE) is a **subword tokenization algorithm** used in NLP to split text into smaller units.  \n",
    "It helps handle **rare words**, **out-of-vocabulary words**, and reduces the vocabulary size.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Steps\n",
    "1. **Add end-of-word marker** to each word (e.g., `</w>`).  \n",
    "2. **Count character frequencies** in the dataset.  \n",
    "3. **Find the most frequent adjacent pair** of symbols.  \n",
    "4. **Merge the pair** into a new symbol.  \n",
    "5. **Repeat steps 3-4** until reaching the desired number of merges or vocabulary size.\n",
    "\n",
    "**the tokenizer used for GPT models also doesn't use an <|unk|> token for outof-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks down words into subword units**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50edab4a-2297-45ff-8a65-aa2a1673fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### will use this library as open ai and will not implement from scratch \n",
    "import tiktoken\n",
    "# Correct encoding for GPT-3.5 / GPT-4\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "996dcc35-fcf3-4310-8590-0d7edbbd0012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9906, 11, 656, 499, 1093, 15600, 30, 220, 100257, 763, 279, 7160, 32735, 7317, 2492, 1073, 1063, 16476, 17826, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e0b68-e581-4dcc-884a-87b5e3cf373c",
   "metadata": {},
   "source": [
    "**return the text back**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9f0f8ea-0ab1-406c-ba7c-3030dfbffac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "string = tokenizer.decode(integers)\n",
    "\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa7876-2473-41e7-88c0-59ec83f221ef",
   "metadata": {},
   "source": [
    "**as we can see it can handel unkown words perficttly much better than our simple tokenizer becuse it go to char level of tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e998fee8-b4ba-43f4-aa54-140ab977ae1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76, 2319, 3690, 86, 1604, 291, 658, 7044, 894]\n",
      "mohamedwaleed elmasry\n"
     ]
    }
   ],
   "source": [
    "#example of unkown word \n",
    "integers = tokenizer.encode(\"mohamedwaleed elmasry\")\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8f0831-74be-41f4-b94d-fc72ea943064",
   "metadata": {},
   "source": [
    "## Input-Target Pairs in LLM Training\n",
    "\n",
    "Input-target pairs are fundamental training examples for Large Language Models (LLMs).\n",
    "Each pair consists of:\n",
    "- An input prompt or context provided to the model\n",
    "- The target output the model should generate in response\n",
    "\n",
    "These pairs form the basis of supervised learning for LLMs, enabling the model to learn\n",
    "the relationship between prompts and appropriate responses through techniques like\n",
    "next-token prediction and teacher forcing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78519fef-a8bf-42ee-9381-33e41c7385d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4943\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as txt_file:\n",
    "       raw_text = txt_file.read()\n",
    "\n",
    "# aplay the BPE encoder\n",
    "full_data = tokenizer.encode(raw_text)\n",
    "#length of our vocabulary\n",
    "print(len(full_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f8d2c-ee2a-4aa1-a7df-14a6a420d75c",
   "metadata": {},
   "source": [
    "**lets create input target pairs variables x, y** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b897508f-a0e5-4c42-a9a9-1431c65d0255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]  -->  473\n",
      "[40, 473]  -->  1846\n",
      "[40, 473, 1846]  -->  2744\n",
      "[40, 473, 1846, 2744]  -->  3463\n",
      "[40, 473, 1846, 2744, 3463]  -->  7762\n"
     ]
    }
   ],
   "source": [
    "x=full_data[:-1]\n",
    "y=full_data[1:]\n",
    "\n",
    "for i in range(1,6):\n",
    "    print(x[:i] ,' --> ' ,y[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9077477-dd8d-4f8f-b5fb-a6d63afa598b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and  -->   established\n",
      " and established  -->   himself\n",
      " and established himself  -->   in\n",
      " and established himself in  -->   a\n",
      " and established himself in a  -->   villa\n"
     ]
    }
   ],
   "source": [
    "# for better sentece x(input) --> y(the llm prediction)\n",
    "x = full_data[50:-1]\n",
    "y = full_data[51:]\n",
    "\n",
    "for i in range(1, 6):\n",
    " \n",
    "    print(tokenizer.decode(x[:i]), ' --> ', tokenizer.decode([y[i-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d11f1-66de-4e21-bc4a-b0637f89c6da",
   "metadata": {},
   "source": [
    "### IMPLEMENTING A DATA LOADER for Efficient Input-Target Pairs Feeding\n",
    "**will used pytorch dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0561d136-b826-4dc9-91a1-c1d5e1b25afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8122340-8f5d-4868-937b-b0619f9766ca",
   "metadata": {},
   "source": [
    "- **step1 : toknize the whole dataset**\n",
    "- **step2 : chunk the dataset into overlapping setenses of max_length size** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae41052c-6ed6-49f1-af6c-51d16a29e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset --> is th pythorch class  \n",
    "class LLMDataset(Dataset):\n",
    "    # stride --> how much we will shift the data if =1 then each time we will shift input text by 1 and predict next word \n",
    "    def __init__(self,txt,toknizer,max_length,stride):\n",
    "        self.input_ids  = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        #toknize the whole data set and add end_of_text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # then devide the data into input and target chunks but as tensors not list\n",
    "        for i in range(0,len(token_ids) - max_length,stride ):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk= token_ids[i + 1 : i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def  __getitem__(self,index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40226ef6-93ad-4713-adc6-44c3b24cf5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Dataloader(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    #intializt our tokinizer \n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    dataset = LLMDataset(txt,tokenizer,max_length,stride)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last,\n",
    "                             num_workers=num_workers)\n",
    "    return dataloader\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac2905-cced-4cf4-b3b4-6aa01fd02ffa",
   "metadata": {},
   "source": [
    "**now lets try this on our data set to see the result** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3eb5f067-684a-4681-b741-186d10e07f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  473, 1846, 2744]]), tensor([[ 473, 1846, 2744, 3463]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as txt_file:\n",
    "       raw_data = txt_file.read()\n",
    "\n",
    "dataloader = Create_Dataloader(raw_data ,batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece4e332-4f6c-428b-95f3-13ef577f7e4f",
   "metadata": {},
   "source": [
    "**these examples for illustration only batch size for nural networks should be at least 256**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b103deef-f2b7-4fc1-99a0-f414cf448769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      " tensor([[   40,   473,  1846,  2744],\n",
      "        [ 3463,  7762,   480,   285],\n",
      "        [22464,  4856,   264, 12136],\n",
      "        [35201,   313,  4636,   264],\n",
      "        [ 1695, 12637,  3403,   313],\n",
      "        [  708,   433,   574,   912],\n",
      "        [ 2294, 13051,   311,   757],\n",
      "        [  311,  6865,   430,    11]]) \n",
      "\n",
      "targets: \n",
      " tensor([[  473,  1846,  2744,  3463],\n",
      "        [ 7762,   480,   285, 22464],\n",
      "        [ 4856,   264, 12136, 35201],\n",
      "        [  313,  4636,   264,  1695],\n",
      "        [12637,  3403,   313,   708],\n",
      "        [  433,   574,   912,  2294],\n",
      "        [13051,   311,   757,   311],\n",
      "        [ 6865,   430,    11,   304]])\n"
     ]
    }
   ],
   "source": [
    "#how batch size change\n",
    "#change stride also to decrease overlab to decrease overfitting\n",
    "dataloader = Create_Dataloader(raw_data ,batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"inputs:\",'\\n',inputs,'\\n')\n",
    "print(\"targets:\",'\\n',targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c402025-a3cf-43e7-8475-410e87ad24a2",
   "metadata": {},
   "source": [
    "## How Embeddings Work\n",
    "\n",
    "Token embeddings are the bridge between text and numbers. They allow machine learning models to understand and operate on language by turning tokens (words, subwords, characters) into vectors.\n",
    "\n",
    "### 1. **Tokenizer Produces Token IDs**\n",
    "\n",
    "* Text is split using a tokenizer (like BPE).\n",
    "* Each token is mapped to an integer ID.\n",
    "\n",
    "### 2. **Embedding Matrix**\n",
    "\n",
    "* A matrix of size:\n",
    "\n",
    "  **vocab_size × embedding_dim**\n",
    "\n",
    "  Example: `50,257 × 768`.\n",
    "* Each row corresponds to the vector representation of a token.\n",
    "* Initialized randomly.\n",
    "\n",
    "### 3. **Lookup Step**\n",
    "\n",
    "* When the model receives token IDs, it uses them to index into the embedding matrix.\n",
    "* Output: A dense vector for each token.\n",
    "\n",
    "### 4. **Learning the Embeddings**\n",
    "\n",
    "* Embeddings update **only when a training objective exists**.\n",
    "\n",
    "* Common objectives:\n",
    "\n",
    "  * Next-token prediction (causal LM)\n",
    "  * Masked-token prediction (BERT)\n",
    "  * Sequence classification (sentiment, etc.)\n",
    "\n",
    "* The embedding vectors change during training because:\n",
    "\n",
    "  * The model predicts something.\n",
    "  * It computes error (loss).\n",
    "  * Backpropagation updates **ALL weights**, including embeddings.\n",
    "\n",
    "### 5. **Embeddings Alone Don’t Train**\n",
    "\n",
    "* If you only feed input tokens and do not ask the model to predict anything, the embeddings never update.\n",
    "* Embeddings need a **supervision signal** or **self-supervised objective**.\n",
    "\n",
    "### 6. **Why Embeddings Matter**\n",
    "\n",
    "* They capture relationships:\n",
    "\n",
    "  * Similar tokens → similar vectors\n",
    "  * Semantic meaning encoded in geometry\n",
    "\n",
    "Embedding quality depends entirely on **training objective + data quantity + model architecture**.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "Token embeddings are random at first. They become meaningful **only because the model is forced to make predictions**, and errors from those predictions update the embedding matrix during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597ba6eb-0f24-4796-8311-1415b817ff1d",
   "metadata": {},
   "source": [
    "**very simple example if we have only 4 token ids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0e34e5f-9a1f-455f-9d40-9d939093d9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 5, 4])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor([2,3,5,4])\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adfad8b7-d434-4d42-9ef3-ea63132479ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we have vocabulary of 6 words \n",
    "vocab_size =6\n",
    "#and we will create an embiding vector of dimnsion of only 3\n",
    "output_embiding=3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "#crate the layer which will produce our vectos \n",
    "embedding_layer = torch.nn.Embedding(vocab_size,output_embiding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6728e8df-4259-4a10-95d6-bae53aabc152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#show the whole embding matrix >> our vocabulary_size * vecor dimnsion\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a80d30-84f9-42f1-996d-b4c04b6dba6f",
   "metadata": {},
   "source": [
    "**thes wheights need to optimize in training process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5127ad14-e75f-4001-9add-24f1e5d7fdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [-1.1589,  0.3255, -0.6315]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#now lets convert our input ids into token embeddings\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb747f4-bf31-4484-8c6e-d97473f44838",
   "metadata": {},
   "source": [
    "## Absolute Positional Embeddings\n",
    "\n",
    "Absolute positional embeddings give the model information about **the position of each token in the sequence**. Since token embeddings alone do not contain any notion of order, positional embeddings are added to preserve sequence structure.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Why Do We Need Positional Embeddings?**\n",
    "\n",
    "Transformers process tokens in parallel (unlike RNNs). Therefore:\n",
    "\n",
    "* The model sees all tokens at once.\n",
    "* It has **no natural sense of order**.\n",
    "\n",
    "Positional embeddings inject the ordering information needed for:\n",
    "\n",
    "* understanding sentences\n",
    "* recognizing the difference between \"Alice loves Bob\" and \"Bob loves Alice\"\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Embedding Shape**\n",
    "\n",
    "If:\n",
    "\n",
    "* sequence length = *N*\n",
    "* embedding dimension = *d*\n",
    "\n",
    "Then positional embedding matrix has size:\n",
    "\n",
    "**N × d**\n",
    "\n",
    "Example:\n",
    "\n",
    "* maximum sequence length = 1024\n",
    "* embedding dimension = 768\n",
    "\n",
    "Positional embeddings shape:\n",
    "\n",
    "**1024 × 768**\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **How Absolute Positional Embeddings Work**\n",
    "\n",
    "Each position *i* (0, 1, 2, ..., N−1) has a unique vector:\n",
    "\n",
    "```text\n",
    "pos_embedding[i] → vector of size d\n",
    "```\n",
    "\n",
    "For each token embedding:\n",
    "\n",
    "```text\n",
    "final_embedding[i] = token_embedding[i] + pos_embedding[i]\n",
    "```\n",
    "\n",
    "Token embedding = meaning of the word\n",
    "Positional embedding = where it appears\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Initialization and Learning**\n",
    "\n",
    "Absolute positional embeddings are typically:\n",
    "\n",
    "* initialized randomly\n",
    "* updated during training via backpropagation\n",
    "\n",
    "They learn patterns like:\n",
    "\n",
    "* beginnings of sentences\n",
    "* typical word orders\n",
    "* location-based semantics (e.g., punctuation at sequence end)\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Summary**\n",
    "\n",
    "* Absolute positional embeddings provide fixed vectors for each position.\n",
    "* Added directly to token embeddings.\n",
    "* Learn during training.\n",
    "* Simple but limited for long or flexible sequences.\n",
    "\n",
    "They were used in models like the original **Transformer** and **GPT-2**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ed27da5-708a-4d6c-8474-c4835d0dfae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try a real example of this \n",
    "vocab_size = 50257 #like GPT-2\n",
    "output_dim = 256\n",
    "\n",
    "embedding_layer = torch.nn.Embedding(vocab_size , output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "718581b9-c094-4cb5-8ca9-0524dcd6a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use data loader to create the token ids \n",
    "max_length = 4\n",
    "batch_size = 8\n",
    "\n",
    "data_loader = Create_Dataloader(raw_data,batch_size=batch_size,max_length=max_length,stride=max_length,shuffle=False)\n",
    "\n",
    "first_data = iter(data_loader)\n",
    "inputs, targets =next(first_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56e6a0dc-c1e2-4088-83f8-59353ae2362b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   473,  1846,  2744],\n",
      "        [ 3463,  7762,   480,   285],\n",
      "        [22464,  4856,   264, 12136],\n",
      "        [35201,   313,  4636,   264],\n",
      "        [ 1695, 12637,  3403,   313],\n",
      "        [  708,   433,   574,   912],\n",
      "        [ 2294, 13051,   311,   757],\n",
      "        [  311,  6865,   430,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c566537-2ac6-4aa2-ac67-f0b33596bb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340ed93-5472-44fd-a663-beaaf9f4a5e4",
   "metadata": {},
   "source": [
    "**now the postional encoding layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9807cdd1-67db-4eae-a95a-b9202ce4ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create this only one time as our context size * our output vector dim \n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3b76357-e196-4e22-be47-d428820a0e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "# create 0,1,...,context size \n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99cc3589-76b9-4aac-a78b-ab08e85c11a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# add same pos_embedding to each context  to get final input training for llm \n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "panel-cell-order": [
   "3c656576",
   "b5541d83",
   "4a2240bd-8453-45c7-a3f6-99ee07d9d5e6"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
